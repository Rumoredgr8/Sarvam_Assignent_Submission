{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heNE2ef-O_m-",
        "outputId": "54c0484d-9775-4dd8-bb74-5865b1bae322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313501 sha256=7ac78db8f1d21ca88e03d603d042af0934cd0f893807a1eebf7d4598fbd601b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o92Hb6E1ioOI",
        "outputId": "90b71148-43ec-4a48-b144-ed30ace57701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O en-hi.txt https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kM02aZhlFY5",
        "outputId": "1db244ec-37f6-42e4-e1bf-14fbfb0ef5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-01 17:59:26--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.169.121.110, 3.169.121.57, 3.169.121.81, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.169.121.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930856 (909K) [text/x-c++]\n",
            "Saving to: ‘en-hi.txt’\n",
            "\n",
            "en-hi.txt           100%[===================>] 909.04K  1.85MB/s    in 0.5s    \n",
            "\n",
            "2025-04-01 17:59:27 (1.85 MB/s) - ‘en-hi.txt’ saved [930856/930856]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh en-hi.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc6XXMxQlNSX",
        "outputId": "a288f4ab-b5c1-44be-ba09-04fb569afe11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 910K Jan 31  2019 en-hi.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -3 \"/content/drive/MyDrive/MUSE/wiki.hi.vec\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD0NXA8Rg8wc",
        "outputId": "0e257210-f667-4fe3-fa93-884ffbb589b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "158016 300\n",
            "</s> 0.11805 0.020802 -0.1089 -0.15894 -0.099375 0.18186 0.10915 -0.13444 0.088495 0.13883 0.0087136 0.000868 -0.15426 -0.11779 -0.19404 -0.09652 -0.0016641 0.018956 -0.061415 -0.067655 0.096204 0.11215 -0.032286 0.47692 0.30003 -0.045058 0.071621 -0.03294 0.15929 -0.20687 -0.116 0.11844 -0.04063 -0.11685 0.05468 0.011329 0.10962 -0.12171 -0.030407 -0.048362 0.12066 -0.089797 0.037743 -0.17441 0.16022 0.0015027 0.082437 -0.11796 0.14972 0.043381 0.010314 -0.039055 -0.16809 -0.075393 0.064941 0.016056 -0.022792 -0.0080583 0.13621 -0.070592 0.18616 0.24617 0.16976 0.14741 0.1235 -0.12475 -0.11184 0.14197 -0.24451 0.011281 0.23877 -0.14358 0.081527 0.061886 0.043097 0.15992 0.023049 -0.085767 -0.025619 0.15381 -0.28751 0.040967 -0.18054 0.10728 0.11467 -0.0030426 0.037164 -0.053903 0.14863 -0.076724 -0.013779 -0.0013568 0.18816 0.26511 -0.10453 -0.011901 0.13636 0.0047142 0.19787 0.030574 -0.059869 0.073519 0.12941 0.0089557 -0.057192 0.22919 -0.08126 -0.049841 -0.0011047 -0.013452 -0.12293 -0.010695 -0.028452 -0.14755 -0.061832 0.040478 0.24521 0.062966 -0.037419 0.13168 0.013332 -0.13451 0.15733 0.033842 -0.068563 -0.1613 0.051322 0.24067 -0.081182 -0.048671 -0.049525 0.051927 0.11702 -0.095872 0.099639 0.033091 -0.097333 0.002716 0.062646 -0.047947 -0.030394 0.098726 -0.18247 0.39785 0.025409 -0.28601 -0.041362 0.027969 -0.15657 -0.034676 -0.1753 -0.11482 0.2442 0.01965 -0.11692 -0.02017 0.14562 -0.054055 -0.10563 -0.051546 0.026419 -0.24778 0.11209 0.073312 -0.1085 0.18497 0.18852 0.12909 0.056415 0.036135 -0.065201 0.12205 -0.061387 0.0057911 0.091134 0.032861 -0.076805 0.085978 -0.011301 0.23264 -0.10083 0.087068 -0.16437 0.066917 0.12631 0.068206 0.11992 -0.038507 -0.010739 -0.049057 0.084083 -0.1687 0.11436 -0.028431 0.25931 -0.026003 -0.012446 -0.075118 -0.086255 -0.11638 0.24485 0.1103 0.018647 -0.05088 -0.21728 -0.067989 -0.20231 0.10583 -0.16496 0.060035 -0.076354 0.12542 0.32954 0.012297 0.13003 -0.071078 -0.12413 0.07397 0.08882 -0.09619 0.15867 -0.13421 -0.0182 0.089019 -0.10958 0.084224 -0.012008 0.029284 0.081788 -0.071359 0.1673 0.054813 0.047775 -0.035531 -0.36426 -0.090726 0.064002 0.19477 -0.072596 0.066601 -0.093907 0.036253 0.0424 -0.081978 0.0072097 -0.09951 0.16519 0.049235 0.069836 -0.089731 0.021012 -0.01999 0.0024132 0.05559 -0.06441 -0.072298 0.12433 0.19049 -0.031093 0.13628 0.25453 -0.023912 0.18653 0.11098 0.19976 0.16829 0.12488 -0.096384 0.082534 -0.39943 -0.10087 0.15827 -0.025021 -0.038629 -0.026618 0.0082518 0.045039 0.031201 0.039719 -0.022482 -0.040601 0.007275 0.086907 -0.099592 -0.020448 -0.1583 -0.25275 0.19251 -0.22191 -0.16235 -0.074462 -0.32979 0.24958 -0.36434 -0.13021 0.067915 -0.086557 0.15536 0.029688 -0.066381 \n",
            "के 0.046271 -0.062825 -0.13019 -0.081064 0.20132 0.035899 0.054962 -0.021752 0.11351 0.0056929 -0.0027507 -0.0021876 -0.35342 -0.021807 0.10967 0.028848 -0.0086524 -0.035754 0.10435 -0.12547 0.10898 0.089893 0.31709 0.13962 0.1024 0.0047501 -0.037492 0.062767 0.084519 0.20156 0.049685 0.081132 -0.061762 0.16084 0.052234 -0.063994 0.14446 0.02963 0.23153 -0.059726 -0.031571 -0.088169 -0.23371 -0.10874 -0.12587 -0.0064134 -0.041136 -0.11884 0.1608 -0.062858 -0.052549 0.14045 0.13036 0.16015 0.14402 -0.27292 -0.22844 0.036744 -0.13193 0.16101 0.0097727 0.031117 0.13733 -0.3555 -0.16257 -0.093805 0.14987 0.090278 0.027691 0.12396 -0.1318 0.01174 0.061346 0.25198 0.17016 0.2343 0.060047 0.054009 -0.011722 -0.14366 -0.058397 0.031628 0.24295 0.0021952 -0.049459 0.018947 -0.1344 -0.17059 -0.11356 -0.050991 -0.045381 0.061151 0.17616 -0.11476 -0.36281 0.34356 0.044978 0.0038132 0.21765 0.20969 0.28726 -0.34711 0.034953 -0.034216 0.055668 0.069102 0.00097159 0.16138 -0.18089 -0.027154 0.020581 0.02302 0.15546 -0.030402 -0.16244 -0.027099 0.27806 -0.17818 -0.022401 0.13987 -0.033381 -0.31055 -0.16008 -0.17425 0.19513 -0.080381 -0.01686 0.16823 -0.16685 -0.17748 0.27112 -0.02775 -0.053573 0.084899 0.15946 -0.11713 -0.20621 -0.016887 -0.093034 0.048873 0.098309 -0.029222 -0.015351 0.12399 0.068147 -0.067998 0.077759 0.0831 0.0087459 -0.17638 -0.13478 -0.20141 0.068796 0.10764 0.23866 -0.11996 -0.089175 0.102 -0.19012 -0.24348 -0.17322 0.12414 -0.23107 -0.0723 -0.2318 -0.074264 -0.019518 0.057754 0.090643 0.19654 -0.035356 0.19224 0.0053865 -0.076495 -0.21222 0.057828 0.15427 0.06159 0.29416 -0.062909 -0.095581 -0.067993 -0.10573 0.091784 0.16112 -0.0010648 -0.033611 0.11894 -0.18186 0.041104 0.11957 -0.05658 0.039857 -0.17225 0.14969 -0.083594 0.032778 -0.13911 0.072807 0.12004 0.17656 0.065323 0.14094 -0.0028454 -0.16053 0.064354 -0.15476 -0.052189 -0.078889 -0.10347 -0.077149 0.30559 0.12067 -0.007971 0.13497 -0.11294 0.16893 -0.14694 0.24794 -0.038556 -0.22018 0.0015592 -0.049867 -0.016174 0.23175 0.012347 0.047669 -0.0033997 0.33198 0.034026 -0.018019 0.14589 -0.15403 0.25513 0.16142 -0.013901 0.090625 -0.14326 -0.097925 0.068488 -0.1907 -0.18779 0.0034585 0.014532 0.18627 -0.38217 0.11262 0.091562 0.14482 -0.059441 -0.19399 -0.0043234 0.064165 -0.033052 0.064483 0.14428 -0.10053 0.13084 -0.053325 -0.031533 0.20578 -0.15901 0.20192 -0.052553 0.013085 0.10957 0.19477 0.12617 -0.093654 -0.21978 0.22133 -0.065065 0.1744 -0.066733 -0.056708 -0.10188 0.033114 0.0013466 0.093737 0.12994 0.024208 -0.2559 -0.012882 -0.11019 -0.15171 -0.029717 0.05062 0.035161 0.039687 -0.23534 -0.050527 0.074928 0.010987 -0.041115 0.10597 -0.056739 -0.12424 -0.12146 0.1547 -0.014931 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install numpy scipy\n",
        "\n",
        "# Import Libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from scipy.spatial.distance import cdist\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Step 2: Define File Paths (Modify these paths if needed)\n",
        "EN_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.en.vec\"  # English embeddings\n",
        "HI_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.hi.vec\"  # Hindi embeddings\n",
        "LEXICON_PATH = \"en-hi.txt\"  # We already downloaded this in Colab\n",
        "\n",
        "# Step 3: Load Pre-trained Word Embeddings\n",
        "def load_muse_embeddings(file_path, max_vocab=50000):\n",
        "    word2vec = {}\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        first_line = f.readline()  # Skip header line\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_vocab:\n",
        "                break\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                vec = np.array(parts[1:], dtype=np.float32)  # Convert only the vector part\n",
        "                word2vec[word] = vec\n",
        "            except ValueError:\n",
        "                print(f\"Skipping malformed line {i+2}: {line}\")\n",
        "    return word2vec\n",
        "\n",
        "# Step 4: Load Bilingual Lexicon\n",
        "def load_bilingual_lexicon(file_path, max_pairs=10000):\n",
        "    lexicon = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_pairs:\n",
        "                break\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            lexicon.append((en_word, hi_word))\n",
        "    return lexicon\n",
        "\n",
        "#  Step 5: Extract Embeddings for Words in the Lexicon\n",
        "def extract_embedding_matrix(lexicon, src_embeddings, tgt_embeddings):\n",
        "    src_vectors, tgt_vectors = [], []\n",
        "    valid_pairs = []\n",
        "    for en_word, hi_word in lexicon:\n",
        "        if en_word in src_embeddings and hi_word in tgt_embeddings:\n",
        "            src_vectors.append(src_embeddings[en_word])\n",
        "            tgt_vectors.append(tgt_embeddings[hi_word])\n",
        "            valid_pairs.append((en_word, hi_word))  # Keep only valid pairs\n",
        "    return np.array(src_vectors), np.array(tgt_vectors), valid_pairs\n",
        "\n",
        "# Step 6: Perform Procrustes Alignment\n",
        "def procrustes_alignment(X, Y):\n",
        "    W, _ = orthogonal_procrustes(X, Y)  # Find optimal rotation matrix\n",
        "    return W\n",
        "\n",
        "# Step 7: Find Nearest Neighbors\n",
        "def nearest_neighbors(src_emb, tgt_emb, k=5):\n",
        "    distances = cdist(src_emb, tgt_emb, metric='cosine')  # Compute cosine distances\n",
        "    return np.argsort(distances, axis=1)[:, :k]  # Get k nearest indices\n",
        "\n",
        "# Step 8: Evaluate Translation Accuracy\n",
        "def evaluate_translation(valid_pairs, src_embeddings, tgt_embeddings, W, top_k=5):\n",
        "    src_words, tgt_words = zip(*valid_pairs)\n",
        "\n",
        "    # Get embeddings for the valid word pairs\n",
        "    src_emb = np.array([src_embeddings[w] for w in src_words])\n",
        "    tgt_emb = np.array([tgt_embeddings[w] for w in tgt_words])\n",
        "\n",
        "    # Apply alignment transformation\n",
        "    aligned_src_emb = src_emb @ W\n",
        "\n",
        "    # Get nearest neighbors\n",
        "    nn_indices = nearest_neighbors(aligned_src_emb, tgt_emb, k=top_k)\n",
        "\n",
        "    # Compute Precision@1 and Precision@5\n",
        "    precision_at_1 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:1]] for i in range(len(tgt_words))])\n",
        "    precision_at_5 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:5]] for i in range(len(tgt_words))])\n",
        "\n",
        "\n",
        "    return precision_at_1, precision_at_5\n",
        "\n",
        "# Step 9: Run the Entire Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Pre-trained MUSE Embeddings\n",
        "    print(\"Loading embeddings...\")\n",
        "    en_embeddings = load_muse_embeddings(EN_EMBEDDINGS_PATH)\n",
        "    hi_embeddings = load_muse_embeddings(HI_EMBEDDINGS_PATH)\n",
        "\n",
        "    # Load Bilingual Lexicon\n",
        "    print(\"Loading bilingual lexicon...\")\n",
        "    lexicon = load_bilingual_lexicon(LEXICON_PATH, max_pairs=10000)\n",
        "\n",
        "    # Extract embeddings for words in the lexicon\n",
        "    print(\"Extracting embeddings for alignment...\")\n",
        "    X, Y, valid_pairs = extract_embedding_matrix(lexicon, en_embeddings, hi_embeddings)\n",
        "\n",
        "    # Perform Procrustes Alignment\n",
        "    print(\"Performing Procrustes alignment...\")\n",
        "    W = procrustes_alignment(X, Y)\n",
        "\n",
        "    # Evaluate translation accuracy\n",
        "    print(\"Evaluating translation accuracy...\")\n",
        "    p_at_1, p_at_5 = evaluate_translation(valid_pairs, en_embeddings, hi_embeddings, W)\n",
        "\n",
        "    # Print Results\n",
        "    print(f\"Precision@1: {p_at_1 * 100:.2f}%\")\n",
        "    print(f\"Precision@5: {p_at_5 * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhcYxjhdR8z1",
        "outputId": "e0097bd4-c766-4992-cd2e-3bbcb091e1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Loading embeddings...\n",
            "Loading bilingual lexicon...\n",
            "Extracting embeddings for alignment...\n",
            "Performing Procrustes alignment...\n",
            "Evaluating translation accuracy...\n",
            "Precision@1: 42.53%\n",
            "Precision@5: 69.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def compute_cosine_similarity(src_emb, tgt_emb, W):\n",
        "    # Apply alignment transformation to the source embeddings\n",
        "    aligned_src_emb = src_emb @ W\n",
        "\n",
        "    # Compute cosine similarities between aligned source embeddings and target embeddings\n",
        "    similarities = np.array([[1 - cosine(src, tgt) for tgt in tgt_emb] for src in aligned_src_emb])\n",
        "    return similarities\n",
        "\n",
        "# Adding this to the pipeline after evaluating translation accuracy\n",
        "if __name__ == \"__main__\":\n",
        "    # Existing code...\n",
        "    # Evaluate translation accuracy\n",
        "    print(\"Evaluating translation accuracy...\")\n",
        "    p_at_1, p_at_5 = evaluate_translation(valid_pairs, en_embeddings, hi_embeddings, W)\n",
        "\n",
        "    # Compute and display cosine similarities\n",
        "    print(\"Computing cosine similarities...\")\n",
        "    src_words, tgt_words = zip(*valid_pairs)\n",
        "    src_emb = np.array([en_embeddings[w] for w in src_words])\n",
        "    tgt_emb = np.array([hi_embeddings[w] for w in tgt_words])\n",
        "    similarities = compute_cosine_similarity(src_emb, tgt_emb, W)\n",
        "\n",
        "    # Optionally print a few similarity results\n",
        "    print(f\"Sample cosine similarities: {similarities[:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXWdJznJkvao",
        "outputId": "17c5cefd-b70a-4c61-af46-a585bb3107ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating translation accuracy...\n",
            "Computing cosine similarities...\n",
            "Sample cosine similarities: [[0.7445441  0.39032638 0.41594315 ... 0.2965145  0.28866065 0.23254883]\n",
            " [0.36138368 0.6293223  0.54573894 ... 0.26155645 0.23415339 0.22959673]\n",
            " [0.36138368 0.6293223  0.54573894 ... 0.26155645 0.23415339 0.22959673]\n",
            " [0.4783002  0.33716756 0.29790092 ... 0.18006063 0.28276563 0.2231099 ]\n",
            " [0.5009129  0.4919359  0.4435017  ... 0.15416837 0.262429   0.30277038]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install numpy scipy\n",
        "\n",
        "# Import Libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from scipy.spatial.distance import cdist\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "#  Step 2: Define File Paths (Modify these paths if needed)\n",
        "EN_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.en.vec\"  # English embeddings\n",
        "HI_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.hi.vec\"  # Hindi embeddings\n",
        "LEXICON_PATH = \"en-hi.txt\"  # We already downloaded this in Colab\n",
        "\n",
        "#  Step 3: Load Pre-trained Word Embeddings\n",
        "def load_muse_embeddings(file_path, max_vocab=50000):\n",
        "    word2vec = {}\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        first_line = f.readline()  # Skip header line\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_vocab:\n",
        "                break\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                vec = np.array(parts[1:], dtype=np.float32)  # Convert only the vector part\n",
        "                word2vec[word] = vec\n",
        "            except ValueError:\n",
        "                print(f\"Skipping malformed line {i+2}: {line}\")\n",
        "    return word2vec\n",
        "\n",
        "#  Step 4: Load Bilingual Lexicon\n",
        "def load_bilingual_lexicon(file_path, max_pairs=10000):\n",
        "    lexicon = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_pairs:\n",
        "                break\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            lexicon.append((en_word, hi_word))\n",
        "    return lexicon\n",
        "\n",
        "#  Step 5: Extract Embeddings for Words in the Lexicon\n",
        "def extract_embedding_matrix(lexicon, src_embeddings, tgt_embeddings):\n",
        "    src_vectors, tgt_vectors = [], []\n",
        "    valid_pairs = []\n",
        "    for en_word, hi_word in lexicon:\n",
        "        if en_word in src_embeddings and hi_word in tgt_embeddings:\n",
        "            src_vectors.append(src_embeddings[en_word])\n",
        "            tgt_vectors.append(tgt_embeddings[hi_word])\n",
        "            valid_pairs.append((en_word, hi_word))  # Keep only valid pairs\n",
        "    return np.array(src_vectors), np.array(tgt_vectors), valid_pairs\n",
        "\n",
        "#  Step 6: Perform Procrustes Alignment\n",
        "def procrustes_alignment(X, Y):\n",
        "    W, _ = orthogonal_procrustes(X, Y)  # Find optimal rotation matrix\n",
        "    return W\n",
        "\n",
        "#  Step 7: Find Nearest Neighbors\n",
        "def nearest_neighbors(src_emb, tgt_emb, k=5):\n",
        "    distances = cdist(src_emb, tgt_emb, metric='cosine')  # Compute cosine distances\n",
        "    return np.argsort(distances, axis=1)[:, :k]  # Get k nearest indices\n",
        "\n",
        "#  Step 8: Evaluate Translation Accuracy\n",
        "def evaluate_translation(valid_pairs, src_embeddings, tgt_embeddings, W, top_k=5):\n",
        "    src_words, tgt_words = zip(*valid_pairs)\n",
        "\n",
        "    # Get embeddings for the valid word pairs\n",
        "    src_emb = np.array([src_embeddings[w] for w in src_words])\n",
        "    tgt_emb = np.array([tgt_embeddings[w] for w in tgt_words])\n",
        "\n",
        "    # Apply alignment transformation\n",
        "    aligned_src_emb = src_emb @ W\n",
        "\n",
        "    # Get nearest neighbors\n",
        "    nn_indices = nearest_neighbors(aligned_src_emb, tgt_emb, k=top_k)\n",
        "\n",
        "    # Compute Precision@1 and Precision@5\n",
        "    precision_at_1 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:1]] for i in range(len(tgt_words))])\n",
        "    precision_at_5 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:5]] for i in range(len(tgt_words))])\n",
        "\n",
        "\n",
        "    return precision_at_1, precision_at_5\n",
        "\n",
        "#  Step 9: Run the Entire Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Pre-trained MUSE Embeddings\n",
        "    print(\"Loading embeddings...\")\n",
        "    en_embeddings = load_muse_embeddings(EN_EMBEDDINGS_PATH)\n",
        "    hi_embeddings = load_muse_embeddings(HI_EMBEDDINGS_PATH)\n",
        "\n",
        "    # Load Bilingual Lexicon\n",
        "    print(\"Loading bilingual lexicon...\")\n",
        "    lexicon = load_bilingual_lexicon(LEXICON_PATH, max_pairs=5000)\n",
        "\n",
        "    # Extract embeddings for words in the lexicon\n",
        "    print(\"Extracting embeddings for alignment...\")\n",
        "    X, Y, valid_pairs = extract_embedding_matrix(lexicon, en_embeddings, hi_embeddings)\n",
        "\n",
        "    # Perform Procrustes Alignment\n",
        "    print(\"Performing Procrustes alignment...\")\n",
        "    W = procrustes_alignment(X, Y)\n",
        "\n",
        "    # Evaluate translation accuracy\n",
        "    print(\"Evaluating translation accuracy...\")\n",
        "    p_at_1, p_at_5 = evaluate_translation(valid_pairs, en_embeddings, hi_embeddings, W)\n",
        "\n",
        "    # Print Results\n",
        "    print(f\"Precision@1: {p_at_1 * 100:.2f}%\")\n",
        "    print(f\"Precision@5: {p_at_5 * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXkoph0x1y_k",
        "outputId": "9ef3fb2a-42ac-40d7-c692-276f4d9a4475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Loading embeddings...\n",
            "Loading bilingual lexicon...\n",
            "Extracting embeddings for alignment...\n",
            "Performing Procrustes alignment...\n",
            "Evaluating translation accuracy...\n",
            "Precision@1: 43.76%\n",
            "Precision@5: 77.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install numpy scipy\n",
        "\n",
        "# Import Libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from scipy.spatial.distance import cdist\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Step 2: Define File Paths (Modify these paths if needed)\n",
        "EN_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.en.vec\"  # English embeddings\n",
        "HI_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.hi.vec\"  # Hindi embeddings\n",
        "LEXICON_PATH = \"en-hi.txt\"  # We already downloaded this in Colab\n",
        "\n",
        "#  Step 3: Load Pre-trained Word Embeddings\n",
        "def load_muse_embeddings(file_path, max_vocab=50000):\n",
        "    word2vec = {}\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        first_line = f.readline()  # Skip header line\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_vocab:\n",
        "                break\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                vec = np.array(parts[1:], dtype=np.float32)  # Convert only the vector part\n",
        "                word2vec[word] = vec\n",
        "            except ValueError:\n",
        "                print(f\"Skipping malformed line {i+2}: {line}\")\n",
        "    return word2vec\n",
        "\n",
        "# Step 4: Load Bilingual Lexicon\n",
        "def load_bilingual_lexicon(file_path, max_pairs=20000):\n",
        "    lexicon = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_pairs:\n",
        "                break\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            lexicon.append((en_word, hi_word))\n",
        "    return lexicon\n",
        "\n",
        "#  Step 5: Extract Embeddings for Words in the Lexicon\n",
        "def extract_embedding_matrix(lexicon, src_embeddings, tgt_embeddings):\n",
        "    src_vectors, tgt_vectors = [], []\n",
        "    valid_pairs = []\n",
        "    for en_word, hi_word in lexicon:\n",
        "        if en_word in src_embeddings and hi_word in tgt_embeddings:\n",
        "            src_vectors.append(src_embeddings[en_word])\n",
        "            tgt_vectors.append(tgt_embeddings[hi_word])\n",
        "            valid_pairs.append((en_word, hi_word))  # Keep only valid pairs\n",
        "    return np.array(src_vectors), np.array(tgt_vectors), valid_pairs\n",
        "\n",
        "# Step 6: Perform Procrustes Alignment\n",
        "def procrustes_alignment(X, Y):\n",
        "    W, _ = orthogonal_procrustes(X, Y)  # Find optimal rotation matrix\n",
        "    return W\n",
        "\n",
        "# Step 7: Find Nearest Neighbors\n",
        "def nearest_neighbors(src_emb, tgt_emb, k=5):\n",
        "    distances = cdist(src_emb, tgt_emb, metric='cosine')  # Compute cosine distances\n",
        "    return np.argsort(distances, axis=1)[:, :k]  # Get k nearest indices\n",
        "\n",
        "#  Step 8: Evaluate Translation Accuracy\n",
        "def evaluate_translation(valid_pairs, src_embeddings, tgt_embeddings, W, top_k=5):\n",
        "    src_words, tgt_words = zip(*valid_pairs)\n",
        "\n",
        "    # Get embeddings for the valid word pairs\n",
        "    src_emb = np.array([src_embeddings[w] for w in src_words])\n",
        "    tgt_emb = np.array([tgt_embeddings[w] for w in tgt_words])\n",
        "\n",
        "    # Apply alignment transformation\n",
        "    aligned_src_emb = src_emb @ W\n",
        "\n",
        "    # Get nearest neighbors\n",
        "    nn_indices = nearest_neighbors(aligned_src_emb, tgt_emb, k=top_k)\n",
        "\n",
        "    # Compute Precision@1 and Precision@5\n",
        "    precision_at_1 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:1]] for i in range(len(tgt_words))])\n",
        "    precision_at_5 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:5]] for i in range(len(tgt_words))])\n",
        "\n",
        "\n",
        "    return precision_at_1, precision_at_5\n",
        "\n",
        "# Step 9: Run the Entire Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Pre-trained MUSE Embeddings\n",
        "    print(\"Loading embeddings...\")\n",
        "    en_embeddings = load_muse_embeddings(EN_EMBEDDINGS_PATH)\n",
        "    hi_embeddings = load_muse_embeddings(HI_EMBEDDINGS_PATH)\n",
        "\n",
        "    # Load Bilingual Lexicon\n",
        "    print(\"Loading bilingual lexicon...\")\n",
        "    lexicon = load_bilingual_lexicon(LEXICON_PATH, max_pairs=10000)\n",
        "\n",
        "    # Extract embeddings for words in the lexicon\n",
        "    print(\"Extracting embeddings for alignment...\")\n",
        "    X, Y, valid_pairs = extract_embedding_matrix(lexicon, en_embeddings, hi_embeddings)\n",
        "\n",
        "    # Perform Procrustes Alignment\n",
        "    print(\"Performing Procrustes alignment...\")\n",
        "    W = procrustes_alignment(X, Y)\n",
        "\n",
        "    # Evaluate translation accuracy\n",
        "    print(\"Evaluating translation accuracy...\")\n",
        "    p_at_1, p_at_5 = evaluate_translation(valid_pairs, en_embeddings, hi_embeddings, W)\n",
        "\n",
        "    # Print Results\n",
        "    print(f\"Precision@1: {p_at_1 * 100:.2f}%\")\n",
        "    print(f\"Precision@5: {p_at_5 * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-MapDfR2Bd9",
        "outputId": "4c4cf79f-6874-4f48-b929-4a27fdb040bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Loading embeddings...\n",
            "Loading bilingual lexicon...\n",
            "Extracting embeddings for alignment...\n",
            "Performing Procrustes alignment...\n",
            "Evaluating translation accuracy...\n",
            "Precision@1: 42.53%\n",
            "Precision@5: 69.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install numpy scipy\n",
        "\n",
        "# Import Libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from scipy.spatial.distance import cdist\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "#  Step 2: Define File Paths (Modify these paths if needed)\n",
        "EN_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.en.vec\"  # English embeddings\n",
        "HI_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.hi.vec\"  # Hindi embeddings\n",
        "LEXICON_PATH = \"en-hi.txt\"  # We already downloaded this in Colab\n",
        "\n",
        "#  Step 3: Load Pre-trained Word Embeddings\n",
        "def load_muse_embeddings(file_path, max_vocab=50000):\n",
        "    word2vec = {}\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        first_line = f.readline()  # Skip header line\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_vocab:\n",
        "                break\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                vec = np.array(parts[1:], dtype=np.float32)  # Convert only the vector part\n",
        "                word2vec[word] = vec\n",
        "            except ValueError:\n",
        "                print(f\"Skipping malformed line {i+2}: {line}\")\n",
        "    return word2vec\n",
        "\n",
        "# Step 4: Load Bilingual Lexicon\n",
        "def load_bilingual_lexicon(file_path, max_pairs=10000):\n",
        "    lexicon = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_pairs:\n",
        "                break\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            lexicon.append((en_word, hi_word))\n",
        "    return lexicon\n",
        "\n",
        "#  Step 5: Extract Embeddings for Words in the Lexicon\n",
        "def extract_embedding_matrix(lexicon, src_embeddings, tgt_embeddings):\n",
        "    src_vectors, tgt_vectors = [], []\n",
        "    valid_pairs = []\n",
        "    for en_word, hi_word in lexicon:\n",
        "        if en_word in src_embeddings and hi_word in tgt_embeddings:\n",
        "            src_vectors.append(src_embeddings[en_word])\n",
        "            tgt_vectors.append(tgt_embeddings[hi_word])\n",
        "            valid_pairs.append((en_word, hi_word))  # Keep only valid pairs\n",
        "    return np.array(src_vectors), np.array(tgt_vectors), valid_pairs\n",
        "\n",
        "#  Step 6: Perform Procrustes Alignment\n",
        "def procrustes_alignment(X, Y):\n",
        "    W, _ = orthogonal_procrustes(X, Y)  # Find optimal rotation matrix\n",
        "    return W\n",
        "\n",
        "#  Step 7: Find Nearest Neighbors\n",
        "def nearest_neighbors(src_emb, tgt_emb, k=5):\n",
        "    distances = cdist(src_emb, tgt_emb, metric='cosine')  # Compute cosine distances\n",
        "    return np.argsort(distances, axis=1)[:, :k]  # Get k nearest indices\n",
        "\n",
        "#  Step 8: Evaluate Translation Accuracy\n",
        "def evaluate_translation(valid_pairs, src_embeddings, tgt_embeddings, W, top_k=5):\n",
        "    src_words, tgt_words = zip(*valid_pairs)\n",
        "\n",
        "    # Get embeddings for the valid word pairs\n",
        "    src_emb = np.array([src_embeddings[w] for w in src_words])\n",
        "    tgt_emb = np.array([tgt_embeddings[w] for w in tgt_words])\n",
        "\n",
        "    # Apply alignment transformation\n",
        "    aligned_src_emb = src_emb @ W\n",
        "\n",
        "    # Get nearest neighbors\n",
        "    nn_indices = nearest_neighbors(aligned_src_emb, tgt_emb, k=top_k)\n",
        "\n",
        "    # Compute Precision@1 and Precision@5\n",
        "    precision_at_1 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:1]] for i in range(len(tgt_words))])\n",
        "    precision_at_5 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:5]] for i in range(len(tgt_words))])\n",
        "\n",
        "\n",
        "    return precision_at_1, precision_at_5\n",
        "\n",
        "# Step 9: Run the Entire Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Pre-trained MUSE Embeddings\n",
        "    print(\"Loading embeddings...\")\n",
        "    en_embeddings = load_muse_embeddings(EN_EMBEDDINGS_PATH)\n",
        "    hi_embeddings = load_muse_embeddings(HI_EMBEDDINGS_PATH)\n",
        "\n",
        "    # Load Bilingual Lexicon\n",
        "    print(\"Loading bilingual lexicon...\")\n",
        "    lexicon = load_bilingual_lexicon(LEXICON_PATH, max_pairs=1000)\n",
        "\n",
        "    # Extract embeddings for words in the lexicon\n",
        "    print(\"Extracting embeddings for alignment...\")\n",
        "    X, Y, valid_pairs = extract_embedding_matrix(lexicon, en_embeddings, hi_embeddings)\n",
        "\n",
        "    # Perform Procrustes Alignment\n",
        "    print(\"Performing Procrustes alignment...\")\n",
        "    W = procrustes_alignment(X, Y)\n",
        "\n",
        "    # Evaluate translation accuracy\n",
        "    print(\"Evaluating translation accuracy...\")\n",
        "    p_at_1, p_at_5 = evaluate_translation(valid_pairs, en_embeddings, hi_embeddings, W)\n",
        "\n",
        "    # Print Results\n",
        "    print(f\"Precision@1: {p_at_1 * 100:.2f}%\")\n",
        "    print(f\"Precision@5: {p_at_5 * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9CgwqYU2V9M",
        "outputId": "cc692486-87d9-4a3b-aa5d-60bd140000d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Loading embeddings...\n",
            "Loading bilingual lexicon...\n",
            "Extracting embeddings for alignment...\n",
            "Performing Procrustes alignment...\n",
            "Evaluating translation accuracy...\n",
            "Precision@1: 41.68%\n",
            "Precision@5: 92.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLS Implementation\n"
      ],
      "metadata": {
        "id": "UxgPLnL03UWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install numpy scipy torch\n",
        "\n",
        "# Import Libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from google.colab import drive\n",
        "\n",
        "# Step 1: Load Pre-trained Word Embeddings\n",
        "def load_muse_embeddings(file_path, max_vocab=50000):\n",
        "    word2vec = {}\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        first_line = f.readline()  # Skip header line\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_vocab:\n",
        "                break\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            word = parts[0]\n",
        "            try:\n",
        "                vec = np.array(parts[1:], dtype=np.float32)  # Convert only the vector part\n",
        "                word2vec[word] = vec\n",
        "            except ValueError:\n",
        "                print(f\"Skipping malformed line {i+2}: {line}\")\n",
        "    return word2vec\n",
        "\n",
        "# Step 2: Load Bilingual Lexicon\n",
        "def load_bilingual_lexicon(file_path, max_pairs=10000):\n",
        "    lexicon = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_pairs:\n",
        "                break\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            lexicon.append((en_word, hi_word))\n",
        "    return lexicon\n",
        "\n",
        "# Step 3: Extract Embeddings for Words in the Lexicon\n",
        "def extract_embedding_matrix(lexicon, src_embeddings, tgt_embeddings):\n",
        "    src_vectors, tgt_vectors = [], []\n",
        "    valid_pairs = []\n",
        "    for en_word, hi_word in lexicon:\n",
        "        if en_word in src_embeddings and hi_word in tgt_embeddings:\n",
        "            src_vectors.append(src_embeddings[en_word])\n",
        "            tgt_vectors.append(tgt_embeddings[hi_word])\n",
        "            valid_pairs.append((en_word, hi_word))  # Keep only valid pairs\n",
        "    return np.array(src_vectors), np.array(tgt_vectors), valid_pairs\n",
        "\n",
        "# Step 4: Perform Procrustes Alignment\n",
        "def procrustes_alignment(X, Y):\n",
        "    W, _ = orthogonal_procrustes(X, Y)  # Find optimal rotation matrix\n",
        "    return W\n",
        "\n",
        "# Step 5: Compute Cosine Similarity\n",
        "def csls(src_emb, tgt_emb, k=5):\n",
        "    \"\"\"\n",
        "    Compute the CSLS (Cross-Domain Similarity Local Scaling) score between source and target embeddings.\n",
        "    \"\"\"\n",
        "    # Calculate cosine similarity between all pairs of source and target embeddings\n",
        "    cosine_sim = 1 - cdist(src_emb, tgt_emb, metric='cosine')  # Cosine distance to cosine similarity\n",
        "\n",
        "    # Compute the k nearest neighbors for each source word in target space\n",
        "    src_nn_sim = np.sort(cosine_sim, axis=1)[:, -k:]  # k largest similarities for each source word\n",
        "    tgt_nn_sim = np.sort(cosine_sim, axis=0)[-k:, :]  # k largest similarities for each target word\n",
        "\n",
        "    # Compute CSLS\n",
        "    csls_scores = cosine_sim - (np.mean(src_nn_sim, axis=1)[:, None] + np.mean(tgt_nn_sim, axis=0))\n",
        "    return csls_scores\n",
        "\n",
        "# Nearest neighbors using CSLS\n",
        "def nearest_neighbors_csls(src_emb, tgt_emb, k=5):\n",
        "    csls_scores = csls(src_emb, tgt_emb, k=k)\n",
        "    return np.argsort(csls_scores, axis=1)[:, :k]  # Get k nearest indices\n",
        "\n",
        "# Step 6: Adversarial Training (Discriminator)\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, emb_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(emb_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def adversarial_training(src_emb, tgt_emb, epochs=100, batch_size=32, lr=0.001):\n",
        "    # Convert numpy arrays to torch tensors\n",
        "    src_emb = torch.tensor(src_emb, dtype=torch.float32)\n",
        "    tgt_emb = torch.tensor(tgt_emb, dtype=torch.float32)\n",
        "\n",
        "    # Initialize the discriminator\n",
        "    discriminator = Discriminator(src_emb.shape[1])  # Assume source and target embeddings have the same size\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle source and target embeddings\n",
        "        perm_src = torch.randperm(src_emb.size(0))\n",
        "        perm_tgt = torch.randperm(tgt_emb.size(0))\n",
        "\n",
        "        # Create batches\n",
        "        for i in range(0, len(src_emb), batch_size):\n",
        "            # Create a batch for source and target embeddings\n",
        "            src_batch = src_emb[perm_src[i:i + batch_size]]\n",
        "            tgt_batch = tgt_emb[perm_tgt[i:i + batch_size]]\n",
        "\n",
        "            # Concatenate the source and target batches\n",
        "            inputs = torch.cat([src_batch, tgt_batch], dim=0)\n",
        "            labels = torch.cat([torch.zeros(src_batch.size(0)), torch.ones(tgt_batch.size(0))], dim=0)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = discriminator(inputs)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs.view(-1), labels)\n",
        "\n",
        "            # Backward pass and update the weights\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    return discriminator\n",
        "\n",
        "# Step 7: Evaluate Translation Accuracy\n",
        "def evaluate_translation(valid_pairs, src_embeddings, tgt_embeddings, W=None, discriminator=None, top_k=5, method='procrustes'):\n",
        "    src_words, tgt_words = zip(*valid_pairs)\n",
        "\n",
        "    # Get embeddings for the valid word pairs\n",
        "    src_emb = np.array([src_embeddings[w] for w in src_words])\n",
        "    tgt_emb = np.array([tgt_embeddings[w] for w in tgt_words])\n",
        "\n",
        "    # Apply alignment transformation\n",
        "    if method == 'procrustes':\n",
        "        aligned_src_emb = src_emb @ W  # Apply Procrustes alignment\n",
        "    elif method == 'csls':\n",
        "        aligned_src_emb = src_emb  # For CSLS, no transformation is applied\n",
        "    else:\n",
        "        raise ValueError(\"Unknown method: choose either 'procrustes' or 'csls'\")\n",
        "\n",
        "    # Get nearest neighbors\n",
        "    nn_indices = nearest_neighbors_csls(aligned_src_emb, tgt_emb, k=top_k)\n",
        "\n",
        "    # Compute Precision@1 and Precision@5\n",
        "    precision_at_1 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:1]] for i in range(len(tgt_words))])\n",
        "    precision_at_5 = np.mean([tgt_words[i] in [tgt_words[j] for j in nn_indices[i][:5]] for i in range(len(tgt_words))])\n",
        "\n",
        "    return precision_at_1, precision_at_5\n",
        "\n",
        "# Main Code to Run the Full Pipeline\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Load Pre-trained MUSE Embeddings\n",
        "    print(\"Loading embeddings...\")\n",
        "    EN_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.en.vec\"\n",
        "    HI_EMBEDDINGS_PATH = \"/content/drive/MyDrive/MUSE/wiki.hi.vec\"\n",
        "    LEXICON_PATH = \"en-hi.txt\"  # Adjust path as needed\n",
        "\n",
        "    en_embeddings = load_muse_embeddings(EN_EMBEDDINGS_PATH)\n",
        "    hi_embeddings = load_muse_embeddings(HI_EMBEDDINGS_PATH)\n",
        "\n",
        "    # Step 2: Load Bilingual Lexicon\n",
        "    print(\"Loading bilingual lexicon...\")\n",
        "    lexicon = load_bilingual_lexicon(LEXICON_PATH, max_pairs=10000)\n",
        "\n",
        "    # Step 3: Extract Embeddings for Words in the Lexicon\n",
        "    print(\"Extracting embeddings for alignment...\")\n",
        "    X, Y, valid_pairs = extract_embedding_matrix(lexicon, en_embeddings, hi_embeddings)\n",
        "\n",
        "    # Step 4: Perform Procrustes Alignment\n",
        "    print(\"Performing Procrustes alignment...\")\n",
        "    W_procrustes = procrustes_alignment(X, Y)\n",
        "\n",
        "    # Step 5: Perform CSLS with Adversarial Training\n",
        "    print(\"Performing CSLS with adversarial training...\")\n",
        "    discriminator = adversarial_training(X, Y, epochs=10, batch_size=32, lr=0.001)\n",
        "\n",
        "    # Evaluate translation accuracy for Procrustes\n",
        "    print(\"Evaluating Procrustes translation accuracy...\")\n",
        "    p_at_1_procrustes, p_at_5_procrustes = evaluate_translation(valid_pairs, en_embeddings, hi_embeddings, W=W_procrustes, method='procrustes')\n",
        "\n",
        "    # Evaluate translation accuracy for CSLS\n",
        "    print(\"Evaluating CSLS translation accuracy...\")\n",
        "    p_at_1_csls, p_at_5_csls = evaluate_translation(valid_pairs, en_embeddings, hi_embeddings, discriminator=discriminator, method='csls')\n",
        "\n",
        "    # Print Results\n",
        "    print(f\"Procrustes Precision@1: {p_at_1_procrustes * 100:.2f}%\")\n",
        "    print(f\"Procrustes Precision@5: {p_at_5_procrustes * 100:.2f}%\")\n",
        "    print(f\"CSLS Precision@1: {p_at_1_csls * 100:.2f}%\")\n",
        "    print(f\"CSLS Precision@5: {p_at_5_csls * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXuq_kGf3W88",
        "outputId": "38c40fe6-fa30-4d0f-986d-b3b556285f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Loading embeddings...\n",
            "Loading bilingual lexicon...\n",
            "Extracting embeddings for alignment...\n",
            "Performing Procrustes alignment...\n",
            "Performing CSLS with adversarial training...\n",
            "Epoch 1/10, Loss: 0.0015720524825155735\n",
            "Epoch 2/10, Loss: 5.721146590076387e-05\n",
            "Epoch 3/10, Loss: 3.0109056751825847e-05\n",
            "Epoch 4/10, Loss: 2.762002441158984e-05\n",
            "Epoch 5/10, Loss: 4.8732285904407036e-06\n",
            "Epoch 6/10, Loss: 2.897063495765906e-06\n",
            "Epoch 7/10, Loss: 1.5781847650941927e-06\n",
            "Epoch 8/10, Loss: 2.6267832708981587e-06\n",
            "Epoch 9/10, Loss: 1.7171892068290617e-06\n",
            "Epoch 10/10, Loss: 1.0823194998010877e-06\n",
            "Evaluating Procrustes translation accuracy...\n",
            "Evaluating CSLS translation accuracy...\n",
            "Procrustes Precision@1: 0.00%\n",
            "Procrustes Precision@5: 0.00%\n",
            "CSLS Precision@1: 0.01%\n",
            "CSLS Precision@5: 0.13%\n"
          ]
        }
      ]
    }
  ]
}